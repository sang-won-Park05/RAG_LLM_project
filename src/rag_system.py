# src/rag_system.py
# -*- coding: utf-8 -*-
"""
RAG ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤
- ë²¡í„° ê²€ìƒ‰(Chroma) + LLM ê²°í•©
- ë‚ ì§œ/ì£¼ì°¨ ê°•ì˜ ìš”ì•½ (ë‹¨ì¼ ë‚ ì§œ/ì£¼ì°¨ ëª¨ë‘ ì¸ì‹)
- íšŒê³ ë¡(ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼) ìƒì„±
- ì½”ë“œ ê²€ìƒ‰(ìì—°ì–´ ì˜ë„ ìë™ ì¸ì‹) / ì½”ë“œ ì„¤ëª…
- ë‹¤ì¤‘ ì¿¼ë¦¬ í™•ì¥ + ìˆ˜ë™ ì•™ìƒë¸” + ì˜ë„ ê¸°ë°˜ ìŠ¤ì½”ì–´ë§(ëª¨ë¸+ì£¼ì œ: CNN/RAG/LangChain/Retriever ë“±)
"""

import os
import re
import datetime as dt
from typing import List, Dict, Any, Optional, Tuple

import dateparser

from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# --- Re-ranking ëª¨ë¸ ì„í¬íŠ¸ ---
try:
    from sentence_transformers import CrossEncoder
    RERANKER_AVAILABLE = True
except ImportError:
    RERANKER_AVAILABLE = False
    CrossEncoder = None

# --- BM25 / Ensemble í˜¸í™˜ ì„í¬íŠ¸ (ë²„ì „ë³„ ì•ˆì „ì²˜ë¦¬) ---
BM25Retriever = None
EnsembleRetriever = None
try:
    from langchain.retrievers import BM25Retriever as _BM25A, EnsembleRetriever as _ENS_A
    BM25Retriever = _BM25A
    EnsembleRetriever = _ENS_A
except Exception:
    try:
        from langchain_community.retrievers import BM25Retriever as _BM25B
        BM25Retriever = _BM25B
    except Exception:
        BM25Retriever = None
    try:
        from langchain.retrievers import EnsembleRetriever as _ENS_B
        EnsembleRetriever = _ENS_B
    except Exception:
        EnsembleRetriever = None


class RAGSystem:
    """RAG ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(
        self,
        db_path: str = "./chroma_db",
        embedding_model: str = "google/embeddinggemma-300m",
        llm_model: str = "gpt-4o-mini",
        k: int = 5,
        use_reranking: bool = True,
        rerank_model: str = "dragonkue/bge-reranker-v2-m3-ko",
        rerank_top_k: int = 10
    ):
        self.db_path = db_path
        self.k = k
        self.use_reranking = use_reranking
        self.rerank_top_k = rerank_top_k

        # Re-ranking ëª¨ë¸ ì´ˆê¸°í™”
        self.reranker = None
        if self.use_reranking and RERANKER_AVAILABLE:
            try:
                print(f"Re-ranking ëª¨ë¸ ë¡œë”© ì¤‘: {rerank_model}")
                self.reranker = CrossEncoder(rerank_model)
                print("Re-ranking ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"Re-ranking ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.reranker = None
        elif self.use_reranking and not RERANKER_AVAILABLE:
            print("Warning: sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ re-rankingì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì„ë² ë”©
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # LLM
        self.llm = ChatOpenAI(
            model_name=llm_model,
            temperature=0.1,
            max_tokens=2000
        )

        # ìŠ¤í† ì–´ & ë¦¬íŠ¸ë¦¬ë²„
        self.vectorstore: Optional[Chroma] = None
        self.vector_retriever = None
        self.bm25_retriever = None
        self.ensemble_retriever = None
        self._all_documents: List[Document] = []
        self._load_vectorstore()

        # ì¼ë°˜ Q/A í”„ë¡¬í”„íŠ¸
        self.prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template=(
                "ë‹¤ìŒì€ LLM ê°•ì˜ ìë£Œì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n\n"
                "{context}\n\n"
                "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:\n"
                "ì§ˆë¬¸: {question}\n\n"
                "ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:\n"
                "1. ì œê³µëœ ê°•ì˜ ìë£Œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”\n"
                "2. ì†ŒìŠ¤ ì½”ë“œê°€ í¬í•¨ëœ ê²½ìš°, í•´ë‹¹ íŒŒì¼ëª…ê³¼ ì…€ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš”\n"
                "3. ê°•ì˜ ë‚ ì§œê°€ ì–¸ê¸‰ëœ ê²½ìš°, í•´ë‹¹ ë‚ ì§œë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”\n"
                "4. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  \"ì œê³µëœ ìë£Œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ë§í•˜ì„¸ìš”\n\n"
                "ë‹µë³€:"
            )
        )

    # -------------------------------
    # Vectorstore & Retriever
    # -------------------------------
    def _load_vectorstore(self):
        if not os.path.exists(self.db_path):
            print(f"ë²¡í„° DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.db_path}")
            return
        try:
            self.vectorstore = Chroma(
                persist_directory=self.db_path,
                embedding_function=self.embeddings
            )
            raw = self.vectorstore.get()
            self._all_documents = [
                Document(page_content=c, metadata=m)
                for c, m in zip(raw.get("documents", []), raw.get("metadatas", []))
            ]

            self.vector_retriever = self.vectorstore.as_retriever(search_kwargs={"k": max(self.k, 20)})

            if BM25Retriever and self._all_documents:
                try:
                    self.bm25_retriever = BM25Retriever.from_documents(self._all_documents)
                    self.bm25_retriever.k = max(self.k, 20)
                except Exception as e:
                    print(f"BM25 ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.bm25_retriever = None

            if EnsembleRetriever and self.vector_retriever and self.bm25_retriever:
                try:
                    self.ensemble_retriever = EnsembleRetriever(
                        retrievers=[self.vector_retriever, self.bm25_retriever],
                        weights=[0.7, 0.3]
                    )
                except Exception as e:
                    print(f"EnsembleRetriever ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    self.ensemble_retriever = None

            print("ë²¡í„° ìŠ¤í† ì–´ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

    # -------------------------------
    # Re-ranking ìœ í‹¸
    # -------------------------------
    def _rerank_documents(self, query: str, documents: List[Document], top_k: Optional[int] = None) -> List[Document]:
        """Re-rank documents using CrossEncoder model"""
        if not self.reranker or not documents:
            return documents

        top_k = top_k or self.k

        try:
            # ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
            query_doc_pairs = [(query, doc.page_content) for doc in documents]

            # Re-ranking ì ìˆ˜ ê³„ì‚°
            scores = self.reranker.predict(query_doc_pairs)

            # ì ìˆ˜ì™€ ë¬¸ì„œë¥¼ ìŒìœ¼ë¡œ ë§Œë“¤ì–´ ì •ë ¬
            scored_docs = list(zip(scores, documents))
            scored_docs.sort(key=lambda x: x[0], reverse=True)

            # ìƒìœ„ kê°œ ë¬¸ì„œ ë°˜í™˜
            reranked_docs = [doc for score, doc in scored_docs[:top_k]]

            print(f"Re-ranking ì™„ë£Œ: {len(documents)} -> {len(reranked_docs)} ë¬¸ì„œ")
            return reranked_docs

        except Exception as e:
            print(f"Re-ranking ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return documents[:top_k]

    # -------------------------------
    # ê²€ìƒ‰ ìœ í‹¸ (ìˆ˜ë™ ì•™ìƒë¸” + ìŠ¤ì½”ì–´ë§)
    # -------------------------------
    def _rank_combine(self, vdocs: List[Document], bdocs: List[Document], k: int) -> List[Document]:
        def key_of(d: Document) -> Tuple:
            m = d.metadata or {}
            return (
                m.get("source"),
                m.get("filename"),
                m.get("lecture_date"),
                m.get("cell_index"),
                m.get("chunk_index"),
                hash(d.page_content.strip()),
            )

        scores: Dict[Tuple, float] = {}
        where: Dict[Tuple, Document] = {}

        for rank, d in enumerate(vdocs):
            kkey = key_of(d)
            scores[kkey] = scores.get(kkey, 0.0) + 0.7 / (rank + 1)
            where[kkey] = d
        for rank, d in enumerate(bdocs):
            kkey = key_of(d)
            scores[kkey] = scores.get(kkey, 0.0) + 0.3 / (rank + 1)
            where[kkey] = d

        ordered = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        merged = [where[k] for k, _ in ordered][:k]
        return merged

    def _dedup_docs(self, docs: List[Document], limit: int) -> List[Document]:
        seen = set()
        unique = []
        for d in docs:
            sig = (d.metadata.get("filename"), d.metadata.get("lecture_date"), d.metadata.get("cell_index"), hash(d.page_content.strip()))
            if sig in seen:
                continue
            seen.add(sig)
            unique.append(d)
            if len(unique) >= limit:
                break
        return unique

    def _search_once(self, query: str, k: int) -> List[Document]:
        try:
            # ë” ë§ì€ ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ì„œ re-rankingí•  ì¤€ë¹„
            retrieve_k = self.rerank_top_k if self.use_reranking and self.reranker else k

            if self.ensemble_retriever:
                docs = self.ensemble_retriever.invoke(query)[:retrieve_k]
            else:
                v = self.vector_retriever.invoke(query)[:retrieve_k] if self.vector_retriever else []
                b = self.bm25_retriever.invoke(query)[:retrieve_k] if self.bm25_retriever else []
                if v and b:
                    docs = self._rank_combine(v, b, retrieve_k)
                else:
                    docs = (v or b)[:retrieve_k]

            # Re-ranking ì ìš©
            if self.use_reranking and self.reranker and docs:
                docs = self._rerank_documents(query, docs, k)
            else:
                docs = docs[:k]

            return docs
        except Exception as e:
            print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def _multi_query_search(self, queries: List[str], k_each: int = 20, k_final: int = 20) -> List[Document]:
        bag: List[Document] = []
        for q in queries:
            bag.extend(self._search_once(q, k_each))
        return self._dedup_docs(bag, k_final)

    # -------------------------------
    # ì˜ë„(ëª¨ë¸/ì£¼ì œ) ì¸ì‹ & ë§¤ì¹­
    # -------------------------------
    def _intent_models(self, text: str) -> List[str]:
        """ëª¨ë¸/ì£¼ì œ ì˜ë„ ëª¨ë‘ ê°ì§€(CNN, RAG, LangChain, Retriever ë“±)"""
        t = (text or "").lower()
        intents = []

        # ëª¨ë¸êµ°
        if any(k in t for k in ["cnn", "convolution", "conv2d"]): intents.append("cnn")
        if "rnn" in t: intents.append("rnn")
        if "lstm" in t: intents.append("lstm")
        if "gru" in t: intents.append("gru")
        if any(k in t for k in ["transformer", "attention"]): intents.append("transformer")
        if "bert" in t: intents.append("bert")
        if "gpt" in t: intents.append("gpt")
        if any(k in t for k in ["knn", "k-nearest"]): intents.append("knn")
        if any(k in t for k in ["svm", "svc", "svr", "support vector"]): intents.append("svm")

        # ì£¼ì œêµ°
        if "langchain" in t: intents.append("langchain")
        if "retriever" in t or "bm25" in t: intents.append("retriever")
        if any(k in t for k in ["rag", "retrieval augmented generation"]): intents.append("rag")
        if any(k in t for k in ["vector store", "vectorstore", "chroma", "faiss"]): intents.append("vectorstore")
        if "langgraph" in t: intents.append("langgraph")
        if "openai" in t: intents.append("openai")

        # RNN ë¬¶ìŒ
        if "lstm" in intents and "rnn" not in intents: intents.append("rnn")
        if "gru" in intents and "rnn" not in intents: intents.append("rnn")

        return list(dict.fromkeys(intents))

    def _doc_matches_intent(self, doc: Document, intents: List[str]) -> bool:
        if not intents:
            return True
        text = (doc.page_content or "").lower()
        meta = (doc.metadata or {})
        mtypes = (meta.get("model_types") or "").lower()
        fname = (meta.get("filename") or "").lower()

        def has_any(keys: List[str]) -> bool:
            return any(k in text or k in mtypes or k in fname for k in keys)

        SYNS = {
            # ëª¨ë¸
            "cnn": ["cnn", "conv", "convolution", "conv2d", "maxpool", "resnet", "vgg", "alexnet"],
            "rnn": ["rnn", "recurrent"],
            "lstm": ["lstm"],
            "gru": ["gru"],
            "transformer": ["transformer", "self-attention", "multiheadattention", "encoderlayer", "decoderlayer", "bert", "gpt"],
            "bert": ["bert"],
            "gpt": ["gpt"],
            "knn": ["knn", "k-nearest", "kneighborsclassifier"],
            "svm": ["svm", "svc", "svr", "support vector"],
            # ì£¼ì œ
            "langchain": ["langchain", "lcel", "runnable", "langchain_community", "langchain_openai", "as_retriever", "similarity_search", "invoke", "get_relevant_documents"],
            "retriever": ["retriever", "bm25", "ensemble", "as_retriever", "get_relevant_documents", "similarity_search"],
            "rag": ["rag", "retrieval augmented generation", "retrieval", "context", "vector store", "vectorstore"],
            "vectorstore": ["vectorstore", "vector store", "chroma", "faiss", "persist_directory", "embedding_function"],
            "langgraph": ["langgraph", "stategraph", "node", "edge", "start", "end", "runnable"],
            "openai": ["openai", "chatopenai", "gpt", "openai api"],
        }

        for it in intents:
            if it in SYNS and has_any(SYNS[it]):
                return True
        return False

    def _filter_by_intent(self, docs: List[Document], intents: List[str], min_keep: int = 6) -> List[Document]:
        if not intents:
            return docs
        kept = [d for d in docs if self._doc_matches_intent(d, intents)]
        if len(kept) >= min_keep:
            return kept
        extras = [d for d in docs if d not in kept]
        return kept + extras[: max(0, min_keep - len(kept))]

    # -------------------------------
    # ìŠ¤ì½”ì–´ë§(ë¬¸ì„œ/ì½”ë“œ ê³µìš©)
    # -------------------------------
    def _custom_doc_score(self, query: str, doc: Document, intents: Optional[List[str]] = None, code_mode: bool = False) -> float:
        q = (query or "").lower()
        text = (doc.page_content or "").lower()
        meta = (doc.metadata or {})
        libs = (meta.get("libraries") or "").lower()
        fname = (meta.get("filename") or "").lower()
        mtypes = (meta.get("model_types") or "").lower()

        score = 0.0

        # ê³µí†µ ê°€ì‚°: ëŒ€í‘œ ë¼ì´ë¸ŒëŸ¬ë¦¬/í‚¤ì›Œë“œ
        if any(k in text or k in libs for k in [
            "torch", "pytorch", "tensorflow", "keras", "sklearn",
            "kneighborsclassifier", "svm", "conv2d", "lstm", "gru", "transformer",
            "langchain", "lcel", "as_retriever", "similarity_search", "invoke",
            "vectorstore", "chroma", "faiss", "bm25", "ensemble"
        ]):
            score += 1.5

        # íŒŒì¼ëª… ë§¤ì¹­ ë³´ë„ˆìŠ¤ (ì˜ˆ: rag_*.ipynb, langchain_*.ipynb)
        if any(k in fname for k in ["rag", "langchain", "retriever", "bm25", "chroma", "faiss"]):
            score += 1.0

        # ì½”ë“œ ì…€ ìš°ì„ 
        if code_mode and (meta.get("content_type") == "code"):
            score += 0.7

        # ë”¥ëŸ¬ë‹ ì¿¼ë¦¬ì—ì„œ ì§€ë„/DB ê°ì 
        if any(k in q for k in ["ë”¥ëŸ¬ë‹", "deep learning", "cnn", "rnn", "lstm", "gru", "transformer"]):
            if any(k in text for k in ["folium", "heatmap", "map("]):
                score -= 2.0
            if any(k in text for k in ["sqlite", "sqlite3", " select ", " create table "]):
                score -= 0.5

        # ì„¤ì¹˜/í™˜ê²½ ì»¤ë§¨ë“œ ê°ì  ì†Œí­
        if any(sym in text for sym in ["!pip install", "!apt-get", "%pip install"]):
            score -= 0.4

        # ì˜ë„ ì¼ì¹˜/ë¶ˆì¼ì¹˜ ë³´ì •
        if intents:
            if self._doc_matches_intent(doc, intents):
                score += 3.5
            else:
                # ë‹¤ë¥¸ ì‹ í˜¸ ê°•í•˜ë©´ ê°ì 
                other_by = {
                    "cnn": ["lstm", "gru", "transformer", "bert", "gpt", "svm", "knn"],
                    "rnn": ["cnn", "transformer", "bert", "gpt", "svm", "knn"],
                    "lstm": ["cnn", "transformer", "bert", "gpt", "svm", "knn"],
                    "gru": ["cnn", "transformer", "bert", "gpt", "svm", "knn"],
                    "transformer": ["cnn", "lstm", "gru", "svm", "knn"],
                    "bert": ["cnn", "lstm", "gru", "svm", "knn"],
                    "gpt": ["cnn", "lstm", "gru", "svm", "knn"],
                    "knn": ["cnn", "transformer", "bert", "gpt", "lstm", "gru", "svm"],
                    "svm": ["cnn", "transformer", "bert", "gpt", "lstm", "gru", "knn"],
                    "langchain": ["pandas", "folium", "sqlite", "matplotlib"],  # ë¹„í•µì‹¬
                    "retriever": ["folium", "sqlite"],
                    "rag": ["folium", "sqlite"],
                    "vectorstore": ["folium", "sqlite"],
                    "langgraph": ["folium", "sqlite"],
                }
                flat_others = set()
                for it in intents:
                    flat_others.update(other_by.get(it, []))
                if any(k in text for k in flat_others) and not any(k in text for k in intents):
                    score -= 2.0
                else:
                    score -= 0.8

        # ëª¨ë¸íƒ€ì… ì¼ì¹˜ ë³´ë„ˆìŠ¤
        if mtypes and intents:
            if any(it in mtypes for it in intents):
                score += 1.0

        return score

    # -------------------------------
    # ì§ˆì˜ í™•ì¥ (ì˜ë„ ê¸°ë°˜)
    # -------------------------------
    def _expand_queries(self, text: str) -> List[str]:
        t = text.lower().strip()
        intents = self._intent_models(t)

        exp_map: Dict[str, List[str]] = {
            # ëª¨ë¸
            "cnn": ["convolution", "conv2d", "convolutional", "resnet", "vgg", "cnn ì½”ë“œ", "cnn ì˜ˆì œ"],
            "rnn": ["recurrent", "sequence model", "rnn ì½”ë“œ", "rnn ì˜ˆì œ"],
            "lstm": ["lstm ì½”ë“œ", "lstm ì˜ˆì œ"],
            "gru": ["gru ì½”ë“œ", "gru ì˜ˆì œ"],
            "transformer": ["attention", "encoder layer", "decoder layer", "transformer ì½”ë“œ", "transformer ì˜ˆì œ"],
            "bert": ["bert ì½”ë“œ", "bert fine-tuning"],
            "gpt": ["gpt ì½”ë“œ", "gpt fine-tuning"],
            "knn": ["k-nearest neighbors", "kneighborsclassifier", "knn ì½”ë“œ", "knn ì˜ˆì œ"],
            "svm": ["support vector machine", "svc", "svr", "svm ì½”ë“œ", "svm ì˜ˆì œ"],
            # ì£¼ì œ
            "langchain": ["lcel", "runnable", "langchain_community", "langchain_openai", "as_retriever", "similarity_search", "invoke", "get_relevant_documents"],
            "retriever": ["as_retriever", "get_relevant_documents", "similarity_search", "bm25", "ensemble"],
            "rag": ["retrieval augmented generation", "retriever", "vectorstore", "context", "prompt", "rag ì½”ë“œ", "rag ì˜ˆì œ"],
            "vectorstore": ["chroma", "faiss", "persist_directory", "embedding_function", "vectorstore ì½”ë“œ"],
            "langgraph": ["stategraph", "node", "edge", "start", "end", "runnable", "langgraph ì½”ë“œ"],
            "openai": ["chatopenai", "openai api", "openai ì½”ë“œ"],
        }

        qs = [text]
        if intents:
            for it in intents:
                qs += exp_map.get(it, [])
        else:
            # ì˜ë„ ì—†ì„ ë•Œë§Œ ì¼ë°˜ í™•ì¥
            generic = {"ë”¥ëŸ¬ë‹": ["deep learning", "neural network", "ì‹ ê²½ë§", "torch", "tensorflow", "keras"]}
            for key, adds in generic.items():
                if key in t:
                    qs += adds

        lone_terms = ["cnn", "rnn", "lstm", "gru", "transformer", "knn", "svm", "ë”¥ëŸ¬ë‹", "rag", "langchain", "retriever"]
        if t in lone_terms:
            qs += [f"{t} ì½”ë“œ", f"{t} ì˜ˆì œ", f"{t} êµ¬í˜„", f"{t} ëª¨ë¸", f"{t} í•™ìŠµ"]

        qs = list(dict.fromkeys([q for q in qs if q and q.strip()]))[1:2]
        return qs

    # -------------------------------
    # ë¬¸ì„œ ê²€ìƒ‰ (ì™¸ë¶€ í˜¸ì¶œ)
    # -------------------------------
    def search_documents(self, query: str, filter_metadata: Optional[Dict] = None, k: Optional[int] = None) -> List[Document]:
        if not self.vectorstore:
            return []
        kk = k or self.k
        try:
            if filter_metadata:
                # í•„í„°ê°€ ìˆëŠ” ê²½ìš°, re-rankingì„ ìœ„í•´ ë” ë§ì€ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¨ í›„ re-rank
                retrieve_k = self.rerank_top_k if self.use_reranking and self.reranker else kk
                docs = self.vectorstore.similarity_search(query, k=retrieve_k, filter=filter_metadata)

                # Re-ranking ì ìš©
                if self.use_reranking and self.reranker and docs:
                    docs = self._rerank_documents(query, docs, kk)
                else:
                    docs = docs[:kk]

                return docs
            else:
                return self._search_once(query, kk)
        except Exception as e:
            print(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    # -------------------------------
    # ë‚ ì§œ/ì£¼ì°¨ ìœ í‹¸ & ê°•ì˜ ìš”ì•½/íšŒê³ 
    # -------------------------------
    def _normalize_week_phrase(self, s: str) -> str:
        ord_map = {"ì²«ì§¸":"1","ì²«ë²ˆì§¸":"1","ë‘˜ì§¸":"2","ë‘ë²ˆì§¸":"2","ì…‹ì§¸":"3","ì„¸ë²ˆì§¸":"3","ë„·ì§¸":"4","ë„¤ë²ˆì§¸":"4","ë‹¤ì„¯ì§¸":"5","ë‹¤ì„¯ë²ˆì§¸":"5"}
        for k, v in ord_map.items():
            s = s.replace(f"{k} ì£¼", f"{v}ì£¼").replace(f"{k}ì£¼", f"{v}ì£¼")
        s = re.sub(r"(\d+)\s*ë²ˆì§¸\s*ì£¼", r"\1ì£¼", s)
        s = re.sub(r"(\d+)\s*ì£¼(?!ì°¨)", r"\1ì£¼ì°¨", s)
        return " ".join(s.split()).strip()

    def _extract_date_from_text(self, text: str) -> Optional[str]:
        s = text.strip()
        m = re.search(r"(\d{4}-\d{2}-\d{2})", s)
        if m: return m.group(1)
        m = re.search(r"\b(\d{8})\b", s)
        if m:
            d = m.group(1)
            return f"{d[:4]}-{d[4:6]}-{d[6:8]}"
        m = re.search(r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼", s)
        if m:
            year = dt.datetime.now().year
            month = int(m.group(1)); day = int(m.group(2))
            try:
                return dt.date(year, month, day).strftime("%Y-%m-%d")
            except Exception:
                pass
        parsed = dateparser.parse(s, languages=["ko", "en"])
        if parsed:
            return parsed.strftime("%Y-%m-%d")
        return None

    def _week_range(self, year: int, month: int, week: int) -> Tuple[dt.date, dt.date, List[str]]:
        first = dt.date(year, month, 1)
        first_monday = first + dt.timedelta(days=(7 - first.weekday()) % 7)  # 0=ì›”
        start = first_monday + dt.timedelta(weeks=week - 1)
        end = start + dt.timedelta(days=4)
        days = [(start + dt.timedelta(days=i)).strftime("%Y-%m-%d") for i in range(5)]
        return start, end, days

    def _get_docs_for_dates(self, days: List[str], k_per_day: int = 30) -> Dict[str, List[Document]]:
        grouped: Dict[str, List[Document]] = {d: [] for d in days}
        seen = set()
        for d in days:
            docs = self.search_documents("ê°•ì˜ ë‚´ìš© ìš”ì•½", {"lecture_date": d}, k=k_per_day)
            for doc in docs:
                if doc.metadata.get("lecture_date") != d:
                    continue
                h = hash(doc.page_content.strip())
                if h in seen:
                    continue
                seen.add(h)
                grouped[d].append(doc)
        return grouped

    def _summarize_days(self, label: str, days: List[str], period_str: str = "", k_per_day: int = 30) -> str:
        grouped = self._get_docs_for_dates(days, k_per_day=k_per_day)
        any_docs = any(len(v) > 0 for v in grouped.values())
        if not any_docs:
            return f"{label} ê°•ì˜ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        parts = []
        for d in days:
            docs = grouped.get(d, [])
            if not docs:
                parts.append(f"### {d}\n- ìë£Œ ì—†ìŒ")
                continue
            parts.append(f"### {d}")
            for doc in docs:
                fn = doc.metadata.get("filename", "Unknown")
                cell = doc.metadata.get("cell_index", "")
                snippet = doc.page_content.strip().replace("\n", " ")
                if len(snippet) > 500:
                    snippet = snippet[:500] + " ..."
                parts.append(f"- íŒŒì¼: {fn}  ì…€:{cell}\n  ë‚´ìš©: {snippet}")

        allowed_dates_str = ", ".join(days)
        context = "\n".join(parts)

        prompt = (
            f"{label}{(' ' + period_str) if period_str else ''} ê°•ì˜ ìë£Œ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.\n"
            f"ë°˜ë“œì‹œ ì•„ë˜ 'í—ˆìš© ë‚ ì§œ'ì— í¬í•¨ëœ ë‚ ì§œë§Œ ì–¸ê¸‰í•˜ì„¸ìš”. ë‹¤ë¥¸ ë‚ ì§œëŠ” ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n"
            f"í—ˆìš© ë‚ ì§œ: {allowed_dates_str}\n\n"
            f"{context}\n\n"
            "ìš”ì•½ ì§€ì¹¨:\n"
            "- ë‚ ì§œë³„(YYYY-MM-DD) ì†Œì œëª©ìœ¼ë¡œ ì •ë¦¬ (í—ˆìš© ë‚ ì§œë§Œ)\n"
            "- ê° ë‚ ì§œì— ëŒ€í•´: ë‹¤ë£¬ ì£¼ì œ / ì‹¤ìŠµ ì½”ë“œÂ·ëª¨ë¸ / í•µì‹¬ ê°œë…ì„ ê°„ê²°íˆ\n"
            "- ë¬¸ì„œê°€ ì—†ëŠ” ë‚ ì§œëŠ” 'ìë£Œ ì—†ìŒ' ëª…ì‹œ\n"
            "ì¶œë ¥ í˜•ì‹:\n"
            "1) ë‚ ì§œë³„ í•­ëª© ë‚˜ì—´\n"
            "2) ë§ˆì§€ë§‰ì— 'ì°¸ê³  ìë£Œ'ë¡œ íŒŒì¼ëª… ëª©ë¡ (ìˆìœ¼ë©´)\n"
        )
        try:
            summary = self.llm.invoke(prompt).content
        except Exception as e:
            return f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

        refs = []
        for d in days:
            for doc in grouped.get(d, []):
                fn = doc.metadata.get("filename", "Unknown")
                cell = doc.metadata.get("cell_index", "")
                refs.append((d, fn, str(cell)))
        refs = sorted(list({(d, fn, cell) for (d, fn, cell) in refs}), key=lambda x: (x[0], x[1], x[2]))
        if refs:
            summary += "\n\nì°¸ê³  ìë£Œ\n" + "\n".join([f"{d} {fn}{' (ì…€:'+cell+')' if cell else ''}" for d, fn, cell in refs])
        return summary

    def get_lecture_summary(self, date: str) -> str:
        return self._summarize_days(date, [date], k_per_day=30)

    def get_lecture_summary_any(self, date_text: str) -> str:
        try:
            txt = self._normalize_week_phrase(date_text)
            m = re.search(r'(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì£¼ì°¨', txt)
            if m:
                month, week = int(m.group(1)), int(m.group(2))
                year = dt.datetime.now().year
                start, end, days = self._week_range(year, month, week)
                return self._summarize_days(f"{month}ì›” {week}ì£¼ì°¨", days, f"({start}~{end})", k_per_day=30)

            day = self._extract_date_from_text(txt)
            if not day:
                return f"ë‚ ì§œë¥¼ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {date_text}"
            return self._summarize_days(day, [day], k_per_day=30)
        except Exception as e:
            return f"ê°•ì˜ ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"

    # -------------------------------
    # íšŒê³ ë¡(ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼)
    # -------------------------------
    def _reflect_days(self, label: str, days: List[str], period_str: str = "", k_per_day: int = 30) -> str:
        grouped = self._get_docs_for_dates(days, k_per_day=k_per_day)
        any_docs = any(len(v) > 0 for v in grouped.values())
        if not any_docs:
            return f"{label} ìë£Œê°€ ì—†ì–´ íšŒê³ ë¡ì„ ì‘ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        parts = []
        for d in days:
            docs = grouped.get(d, [])
            if not docs:
                continue
            parts.append(f"### {d}")
            for doc in docs:
                fn = doc.metadata.get("filename", "Unknown")
                cell = doc.metadata.get("cell_index", "")
                snippet = doc.page_content.strip().replace("\n", " ")
                if len(snippet) > 400:
                    snippet = snippet[:400] + " ..."
                parts.append(f"- íŒŒì¼: {fn}  ì…€:{cell}  ë‚´ìš©: {snippet}")
        context = "\n".join(parts)
        allowed = ", ".join(days)

        base_env = os.environ.get("CAMP_START_DATE")
        day_range = ""
        if base_env:
            try:
                base = dt.datetime.strptime(base_env, "%Y-%m-%d").date()
                nums = []
                for d in days:
                    di = dt.datetime.strptime(d, "%Y-%m-%d").date()
                    nums.append((di - base).days + 1)
                nums.sort()
                if nums:
                    day_range = f"DAY{nums[0]}" if len(nums) == 1 else f"DAY{nums[0]}~DAY{nums[-1]}"
            except Exception:
                pass

        title = f"[SKN_AI_CAMP16] {label} íšŒê³ "
        if period_str:
            title += f" {period_str}"
        if day_range:
            title = f"{title}\n\n{day_range}"

        prompt = f"""
ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{label}{(' ' + period_str) if period_str else ''}'ì— ëŒ€í•œ 'íšŒê³ ë¡'ì„ ì‘ì„±í•˜ì„¸ìš”.
ë°˜ë“œì‹œ 'í—ˆìš© ë‚ ì§œ'ì— í¬í•¨ëœ ë‚ ì§œë§Œ ì–¸ê¸‰í•˜ê³ , ì œê³µ ìë£Œì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ì‚¬ì‹¤ë§Œ í™œìš©í•˜ì„¸ìš”.

í—ˆìš© ë‚ ì§œ: {allowed}

ì»¨í…ìŠ¤íŠ¸:
{context}

ì¶œë ¥ í˜•ì‹(ë§ˆí¬ë‹¤ìš´, ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼):
# {title}

## ê°œìš”
- í•œ ì£¼(í˜¹ì€ í•˜ë£¨) ì „ì²´ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

## ë‚ ì§œë³„ í•˜ì´ë¼ì´íŠ¸
- ê° ë‚ ì§œ(YYYY-MM-DD)ë³„ë¡œ:
  - âœ”ï¸ ì˜ ëœ ì (Wins)
  - â— ì–´ë ¤ì› ë˜ ì (Challenges)
  - ğŸ’¡ ë°°ìš´ ì (Lessons)
  - ğŸ”§ ê°œì„  ì•„ì´ë””ì–´
  - â–¶ ë‹¤ìŒ ì•¡ì…˜(Action Items, SMART 1~2ê°œ)

## ì „ë°˜ì  íšŒê³ 
- ì„±ì·¨
- ê°œì„ í•  ì 
- ë¦¬ìŠ¤í¬/ì˜ì¡´ì„±(ìˆìœ¼ë©´)
- ë‹¤ìŒ ì£¼ ê³„íš(SMART 2~3ê°œ)

ê·œì¹™:
- í—ˆìš© ë‚ ì§œ ì™¸ì˜ ë‚ ì§œëŠ” ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ ê²ƒ
- ì¶”ì¸¡ ê¸ˆì§€, ìë£Œ ì—†ìœ¼ë©´ 'ìë£Œ ì—†ìŒ'ì´ë¼ê³  ê°„ë‹¨íˆ í‘œê¸°
"""
        try:
            return self.llm.invoke(prompt).content
        except Exception as e:
            return f"íšŒê³ ë¡ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def get_reflection_any(self, date_text: str) -> str:
        try:
            txt = self._normalize_week_phrase(date_text)
            m = re.search(r'(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì£¼ì°¨', txt)
            if m:
                month, week = int(m.group(1)), int(m.group(2))
                year = dt.datetime.now().year
                start, end, days = self._week_range(year, month, week)
                return self._reflect_days(f"{month}ì›” {week}ì£¼ì°¨", days, f"({start}~{end})", k_per_day=30)

            day = self._extract_date_from_text(txt)
            if not day:
                return f"ë‚ ì§œë¥¼ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {date_text}"
            return self._reflect_days(day, [day], k_per_day=20)
        except Exception as e:
            return f"íšŒê³ ë¡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"

    # -------------------------------
    # ì½”ë“œ ê²€ìƒ‰/ì„¤ëª…
    # -------------------------------
    def _looks_like_code_intent(self, text: str) -> bool:
        t = text.lower().strip()
        model_terms = ["cnn", "rnn", "lstm", "gru", "knn", "svm", "bert", "gpt", "transformer", "ë”¥ëŸ¬ë‹",
                       "rag", "langchain", "retriever", "chroma", "faiss", "bm25", "vectorstore"]
        return (
            ("ì½”ë“œ" in text) or ("ì˜ˆì œ" in text) or ("êµ¬í˜„" in text) or ("ì†ŒìŠ¤" in text) or ("ì‘ì„±" in text)
            or ("ìƒ˜í”Œ" in text) or ("sample" in t) or ("code" in t)
            or t in model_terms
        )

    def _strong_code_search(self, query: str, k: int = 20) -> List[Document]:
        intents = self._intent_models(query)

        # Re-rankingì„ ê³ ë ¤í•œ ê²€ìƒ‰ ìˆ˜ ì¡°ì •
        search_k = max(40, k * 2) if self.use_reranking and self.reranker else max(20, k)

        # 1) ì½”ë“œ ë©”íƒ€ ìš°ì„ 
        code_first = self.search_documents(query, filter_metadata={"content_type": "code"}, k=search_k)

        # 2) ì˜ë„ í™•ì¥
        exp_qs = self._expand_queries(query)
        broad = self._multi_query_search(exp_qs, k_each=search_k, k_final=120)

        # 3) ì½”ë“œë§Œ + í•©ì¹˜ê¸°
        pool = []
        pool.extend(code_first)
        pool.extend([d for d in broad if (d.metadata or {}).get("content_type") == "code"])
        pool = self._dedup_docs(pool, limit=250)

        # 4) ì˜ë„ í•„í„°
        pool = self._filter_by_intent(pool, intents, min_keep=6)

        # 5) ìŠ¤ì½”ì–´ë§
        scored = sorted(pool, key=lambda d: self._custom_doc_score(query, d, intents=intents, code_mode=True), reverse=True)

        # 6) Re-ranking ì ìš© (ìŠ¤ì½”ì–´ë§ í›„)
        if self.use_reranking and self.reranker and scored:
            scored = self._rerank_documents(query, scored, k)
        else:
            scored = scored[:k]

        return scored

    def get_code_snippets(self, query: str) -> List[Dict[str, Any]]:
        code_docs = self._strong_code_search(query, k=20)
        return [
            {
                "content": doc.page_content,
                "filename": doc.metadata.get("filename", "Unknown"),
                "cell_index": doc.metadata.get("cell_index", "Unknown"),
                "lecture_date": doc.metadata.get("lecture_date", "Unknown"),
                "libraries": doc.metadata.get("libraries", "").split(", ") if doc.metadata.get("libraries") else [],
                "model_types": doc.metadata.get("model_types", "").split(", ") if doc.metadata.get("model_types") else []
            }
            for doc in code_docs
        ]

    # -------------------------------
    # Q/A (ë¬¸ì„œ ë¶€ì¡± ì‹œ ì˜ë„ ê¸°ë°˜ ì¬ë­í‚¹)
    # -------------------------------
    def answer_question(self, question: str) -> Dict[str, Any]:
        if not self.vectorstore:
            return {"answer": "ë²¡í„° DBê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € DBë¥¼ êµ¬ì¶•í•´ì£¼ì„¸ìš”.", "sources": [], "metadata": {}}
        try:
            intents = self._intent_models(question)

            # 1ì°¨: ì› ì¿¼ë¦¬
            docs = self.search_documents(question, k=max(self.k, 16))

            # 2ì°¨: í™•ì¥ ì¿¼ë¦¬
            if len(docs) < max(4, self.k):
                exp_qs = self._expand_queries(question)
                more = self._multi_query_search(exp_qs, k_each=18, k_final=60)
                docs = self._dedup_docs(docs + more, limit=max(20, self.k * 4))

            # 3ì°¨: ì˜ë„ í•„í„° & ìŠ¤ì½”ì–´ ì¬ì •ë ¬ (ì„¤ì¹˜ ì»¤ë§¨ë“œ/ë¬´ê´€ ë‚´ìš© í•˜í–¥)
            docs = self._filter_by_intent(docs, intents, min_keep=8)
            docs = sorted(docs, key=lambda d: self._custom_doc_score(question, d, intents=intents, code_mode=False), reverse=True)

            if not docs:
                return {"answer": "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "sources": [], "metadata": {}}

            # ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
            topn = min(len(docs), max(10, self.k))
            context_parts, sources = [], []
            for i, doc in enumerate(docs[:topn]):
                filename = doc.metadata.get("filename", "Unknown")
                content_type = doc.metadata.get("content_type", "Unknown")
                cell_index = doc.metadata.get("cell_index", "")
                context_parts.append(f"[ë¬¸ì„œ {i+1}] {filename} ({content_type})\n{doc.page_content}")
                sources.append({
                    "filename": filename,
                    "content_type": content_type,
                    "cell_index": cell_index,
                    "lecture_date": doc.metadata.get("lecture_date", "Unknown")
                })

            context = "\n\n".join(context_parts)
            prompt = self.prompt_template.format(context=context, question=question)
            response = self.llm.invoke(prompt)
            return {"answer": response.content, "sources": sources, "metadata": {"num_sources": len(docs)}}
        except Exception as e:
            return {"answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", "sources": [], "metadata": {}}

    # -------------------------------
    # ë¼ìš°íŒ…
    # -------------------------------
    def _handle_summary(self, text: str) -> str:
        return self.get_lecture_summary_any(text)

    def _handle_code_search(self, text: str) -> str:
        snippets = self.get_code_snippets(text)
        if not snippets:
            return "ê´€ë ¨ ì½”ë“œ ìŠ¤ë‹ˆí«ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        blocks = []
        for s in snippets[: max(self.k, 5) ]:
            code = s["content"]
            if len(code) > 2500:
                code = code[:2500] + "\n# ... (ìƒëµ)"
            blocks.append(
                f"{s['filename']} (ë‚ ì§œ: {s['lecture_date']}, ì…€: {s['cell_index']})\n\n"
                f"```python\n{code}\n```"
            )
        return "\n\n".join(blocks)

    def _handle_code_explain(self, text: str) -> str:
        return self.explain_code(text)

    def _handle_default(self, text: str) -> str:
        return self.answer_question(text).get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")

    def handle_user_input(self, text: str) -> str:
        s = (text or "").strip()
        if not s:
            return ""

        # ëª…ì‹œì  ì ‘ë‘ì‚¬
        if s.startswith("ì½”ë“œ ê²€ìƒ‰:"):
            return self._handle_code_search(s.split(":", 1)[1].strip())
        if s.startswith("ê°•ì˜ ìš”ì•½:"):
            return self._handle_summary(s.split(":", 1)[1].strip())
        if s.startswith("ì½”ë“œ ì„¤ëª…:"):
            return self._handle_code_explain(s.split(":", 1)[1].strip())
        if s.startswith("íšŒê³ ë¡:"):
            return self.get_reflection_any(s.split(":", 1)[1].strip())

        # íšŒê³ ë¡ ìì—°ì–´
        if ("íšŒê³ ë¡" in s):
            cleaned = (
                s.replace("íšŒê³ ë¡", " ")
                 .replace("ë§Œë“¤ì–´ì¤˜", " ")
                 .replace("ì‘ì„±", " ")
                 .replace("ì ì–´ì¤˜", " ")
                 .replace("ì¨ì¤˜", " ")
                 .replace("í•´ì¤˜", " ")
            )
            cleaned = " ".join(cleaned.split())
            return self.get_reflection_any(cleaned)

        # ì£¼ì°¨ íŒ¨í„´
        normalized = self._normalize_week_phrase(s)
        if re.search(r'\d{1,2}\s*ì›”\s*\d{1,2}\s*ì£¼ì°¨', normalized):
            return self._handle_summary(normalized)

        # ë¬¸ì¥ ì† ë‚ ì§œ â†’ ê°•ì˜ ìš”ì•½
        if self._extract_date_from_text(s):
            return self._handle_summary(s)

        # ì½”ë“œ ì˜ë„
        if self._looks_like_code_intent(s):
            return self._handle_code_search(s)

        # ì¼ë°˜ Q/A
        return self._handle_default(s)

    # -------------------------------
    # ì½”ë“œ ì„¤ëª…
    # -------------------------------
    def explain_code(self, code_content: str, context: str = "") -> str:
        prompt = (
            "ë‹¤ìŒ Python ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ì„¤ëª…í•´ì£¼ì„¸ìš”:\n\n"
            f"{context}\n\n"
            "ì½”ë“œ:\n"
            "```python\n"
            f"{code_content}\n"
            "```\n\n"
            "ì„¤ëª… ì‹œ í¬í•¨í•  ë‚´ìš©:\n"
            "1. ì½”ë“œ ëª©ì ê³¼ ê¸°ëŠ¥\n"
            "2. ì£¼ìš” êµ¬ì„± ìš”ì†Œ\n"
            "3. ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬\n"
            "4. ë™ì‘ ìˆœì„œ\n"
            "5. ì£¼ì˜ì‚¬í•­/ê°œì„ ì \n"
        )
        try:
            return self.llm.invoke(prompt).content
        except Exception as e:
            return f"ì½”ë“œ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"


def main():
    rag = RAGSystem()
    print("== ì£¼ì°¨ ìš”ì•½ ==")
    print(rag.handle_user_input("ê°•ì˜ ìš”ì•½: 6ì›” 3ì£¼ì°¨"))
    print("\n== ë‹¨ì¼ ë‚ ì§œ ìš”ì•½ ==")
    print(rag.handle_user_input("6ì›” 11ì¼ ê°•ì˜ìš”ì•½í•´ì¤˜"))
    print("\n== ì½”ë“œ ê²€ìƒ‰ (ìì—°ì–´) ==")
    print(rag.handle_user_input("CNN ëª¨ë¸ ë§Œë“œëŠ” ì½”ë“œ ì°¾ì•„ì¤˜"))
    print("\n== ì½”ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ ë‹¨ë…) ==")
    print(rag.handle_user_input("KNN"))
    print("\n== ì£¼ì œ ê²€ìƒ‰ (LangChain) ==")
    print(rag.handle_user_input("langchain"))
    print("\n== ì£¼ì œ ê²€ìƒ‰ (RAG) ==")
    print(rag.handle_user_input("RAG"))
    print("\n== ì£¼ì œ ê²€ìƒ‰ (Retriever) ==")
    print(rag.handle_user_input("Retriever"))
    print("\n== íšŒê³ ë¡ ==")
    print(rag.handle_user_input("íšŒê³ ë¡: 7ì›” 3ì£¼ì°¨"))


if __name__ == "__main__":
    main()