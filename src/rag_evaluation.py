"""
RAG ì‹œìŠ¤í…œ í‰ê°€ë¥¼ ìœ„í•œ ì¢…í•©ì ì¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
CustomEvaluatorë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ìœ¼ë¡œ RAG ì„±ëŠ¥ì„ í‰ê°€
"""

import os
import json
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

from src.rag_system import RAGSystem
from src.evaluator import CustomEvaluatior
from src.rag_visualizer import RAGVisualizationDashboard, create_summary_report
from langchain_openai import ChatOpenAI

class RAGEvaluationPipeline:
    def __init__(self, rag_system: RAGSystem, evaluator: CustomEvaluatior, enable_visualization: bool = True):
        """
        RAG í‰ê°€ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Args:
            rag_system: í‰ê°€í•  RAG ì‹œìŠ¤í…œ
            evaluator: CustomEvaluator ì¸ìŠ¤í„´ìŠ¤
            enable_visualization: ì‹œê°í™” ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
        """
        self.rag_system = rag_system
        self.evaluator = evaluator
        self.evaluation_results = []
        self.enable_visualization = enable_visualization

        # ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ì´ˆê¸°í™”
        if self.enable_visualization:
            self.visualizer = RAGVisualizationDashboard()

    def create_test_dataset(self) -> List[Dict[str, Any]]:
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± - ì‹¤ì œ êµìœ¡ ìë£Œ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ í˜„ì‹¤ì ì¸ ì§ˆë¬¸ë“¤
        data/educational_materialsì˜ PDFì™€ notebook íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ì—¬ ìƒì„±
        """
        test_queries = [
            # LangChain & RAG ê´€ë ¨ (2025-08-25 langchain_2.ipynb ê¸°ë°˜)
            {
                "question": "LangChainì—ì„œ í•œêµ­ ì „í†µ ìŒì‹ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ground_truth": "LangChainì—ì„œ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ë ¤ë©´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤(Chroma), ì„ë² ë”© ëª¨ë¸, LLMì„ ì¡°í•©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰ í›„ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ì•¼ í•©ë‹ˆë‹¤. í•œêµ­ ì „í†µ ìŒì‹ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.",
                "query_type": "theory",
                "difficulty": "medium"
            },
            {
                "question": "ì½”ë“œ ê²€ìƒ‰: LangChain Document í´ë˜ìŠ¤ ì‚¬ìš©ë²•",
                "ground_truth": "LangChainì˜ Document í´ë˜ìŠ¤ëŠ” page_contentì™€ metadata ì†ì„±ì„ ê°€ì§€ë©°, í…ìŠ¤íŠ¸ ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤. from langchain.schema import Documentë¡œ importí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "query_type": "code",
                "difficulty": "easy"
            },

            # í† í°í™” ê´€ë ¨ (2025-08-11 BPE_Unigram.ipynb ê¸°ë°˜)
            {
                "question": "BPE(Byte Pair Encoding)ì™€ Unigram í† í°í™” ë°©ë²•ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ground_truth": "BPEëŠ” ë¹ˆë„ê°€ ë†’ì€ ë¬¸ì ìŒì„ ë°˜ë³µì ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ì„œë¸Œì›Œë“œë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ì´ê³ , Unigramì€ í™•ë¥  ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì„œë¸Œì›Œë“œ ë¶„í• ì„ ì°¾ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. BPEëŠ” deterministicí•˜ì§€ë§Œ Unigramì€ probabilistic ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                "query_type": "theory",
                "difficulty": "hard"
            },
            {
                "question": "SentencePiece ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ BPE ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ì½”ë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ground_truth": "SentencePieceì—ì„œ BPE ëª¨ë¸ í•™ìŠµì€ spm.SentencePieceTrainer.train()ì„ ì‚¬ìš©í•˜ë©°, input íŒŒì¼, model_prefix, vocab_size, model_type='bpe' ë“±ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.",
                "query_type": "code",
                "difficulty": "medium"
            },

            # GAN ê´€ë ¨ (2025-09-12 ë©€í‹°ëª¨ë‹¬_GAN_ex1.ipynb ê¸°ë°˜)
            {
                "question": "ì¡°ê±´ë¶€ GAN(Conditional GAN)ì—ì„œ í…ìŠ¤íŠ¸ ì¡°ê±´ì„ ì–´ë–»ê²Œ êµ¬í˜„í•˜ë‚˜ìš”?",
                "ground_truth": "ì¡°ê±´ë¶€ GANì—ì„œëŠ” ì„ë² ë”© ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë ˆì´ë¸”ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³ , ì´ë¥¼ ìƒì„±ìì™€ íŒë³„ìì˜ ì…ë ¥ì— concatenateí•©ë‹ˆë‹¤. nn.Embeddingì„ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•œ í›„ ë…¸ì´ì¦ˆ ë²¡í„°ì™€ ê²°í•©í•©ë‹ˆë‹¤.",
                "query_type": "theory",
                "difficulty": "hard"
            },
            {
                "question": "PyTorchì—ì„œ MNIST ë°ì´í„°ë¡œ GANì„ í•™ìŠµì‹œí‚¬ ë•Œ ì‚¬ìš©í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "ground_truth": "MNIST GAN í•™ìŠµì—ëŠ” ì£¼ë¡œ Binary Cross Entropy Loss(nn.BCELoss)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìƒì„±ìëŠ” íŒë³„ìê°€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ì§„ì§œë¡œ íŒë³„í•˜ë„ë¡ í•™ìŠµí•˜ê³ , íŒë³„ìëŠ” ì§„ì§œì™€ ê°€ì§œë¥¼ êµ¬ë¶„í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.",
                "query_type": "code",
                "difficulty": "medium"
            },

            # ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ (KNN, ì˜í™”ì¶”ì²œ ë“±)
            {
                "question": "KNN ì•Œê³ ë¦¬ì¦˜ì—ì„œ Kê°’ì„ ì„ íƒí•˜ëŠ” ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ground_truth": "KNNì—ì„œ Kê°’ì€ í™€ìˆ˜ë¡œ ì„ íƒí•˜ì—¬ ë™ì ì„ ë°©ì§€í•˜ê³ , êµì°¨ ê²€ì¦ì„ í†µí•´ ìµœì ê°’ì„ ì°¾ìŠµë‹ˆë‹¤. Kê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ë…¸ì´ì¦ˆì— ë¯¼ê°í•˜ê³ , ë„ˆë¬´ í¬ë©´ ê²°ì • ê²½ê³„ê°€ smoothí•´ì ¸ underfittingì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "query_type": "theory",
                "difficulty": "medium"
            },
            {
                "question": "ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ TF-IDFì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ground_truth": "TF-IDFëŠ” ë¬¸ì„œì˜ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ê°€ì¤‘ì¹˜ë¡œ í‘œí˜„í•˜ê³ , ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ë²¡í„° ê°„ì˜ ë°©í–¥ì„±ì„ ì¸¡ì •í•˜ì—¬ ë¬¸ì„œ ê¸¸ì´ì— ì˜í–¥ë°›ì§€ ì•ŠëŠ” ìˆœìˆ˜í•œ ìœ ì‚¬ì„±ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì¥ë¥´, ì¤„ê±°ë¦¬ ë“±ì˜ í…ìŠ¤íŠ¸ ì •ë³´ë¡œ ìœ ì‚¬í•œ ì˜í™”ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "query_type": "theory",
                "difficulty": "medium"
            },
            # í‰ê°€ ì§€í‘œ ê´€ë ¨
            {
                "question": "BLEUì™€ ROUGE í‰ê°€ ì§€í‘œì˜ ì°¨ì´ì ê³¼ ì‚¬ìš© ìš©ë„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ground_truth": "BLEUëŠ” ê¸°ê³„ë²ˆì—­ í’ˆì§ˆ í‰ê°€ì— ì£¼ë¡œ ì‚¬ìš©ë˜ë©° n-gram ì •ë°€ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ROUGEëŠ” ìë™ìš”ì•½ í‰ê°€ì— ì‚¬ìš©ë˜ë©° recall ê¸°ë°˜ìœ¼ë¡œ ì°¸ì¡° ìš”ì•½ê³¼ ìƒì„± ìš”ì•½ ê°„ì˜ ê²¹ì¹˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. BLEUëŠ” ì •ë°€ë„, ROUGEëŠ” ì¬í˜„ìœ¨ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.",
                "query_type": "theory",
                "difficulty": "medium"
            },
            # ë°ì´í„° ì „ì²˜ë¦¬
            {
                "question": "ë°ì´í„° ì „ì²˜ë¦¬ì—ì„œ ì •ê·œí™”(Normalization)ì™€ í‘œì¤€í™”(Standardization)ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ground_truth": "ì •ê·œí™”ëŠ” ë°ì´í„°ë¥¼ 0-1 ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” Min-Max scalingì´ê³ , í‘œì¤€í™”ëŠ” í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ë§Œë“œëŠ” Z-score scalingì…ë‹ˆë‹¤. ì •ê·œí™”ëŠ” ì´ìƒì¹˜ì— ë¯¼ê°í•˜ì§€ë§Œ í‘œì¤€í™”ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ê°•ê±´í•©ë‹ˆë‹¤.",
                "query_type": "theory",
                "difficulty": "medium"
            }
        ]
        return test_queries

    def evaluate_single_query(self, query_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ ì¢…í•© í‰ê°€

        Args:
            query_data: ì¿¼ë¦¬ ì •ë³´ (question, ground_truth, query_type, difficulty)

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        question = query_data["question"]
        ground_truth = query_data["ground_truth"]

        # 1. RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±
        response = self.rag_system.handle_user_input(question)

        # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = self.rag_system.ensemble_retriever.get_relevant_documents(question)
        doc_contents = [doc.page_content for doc in retrieved_docs[:5]]  # ìƒìœ„ 5ê°œ ë¬¸ì„œ

        # 3. CustomEvaluatorë¡œ ì‘ë‹µ í’ˆì§ˆ í‰ê°€
        response_evaluation = self.evaluator.evaluate_response(
            query=question,
            response=response,
            ground_truth=ground_truth
        )

        # 4. ê²€ìƒ‰ëœ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€
        retrieval_evaluation = self.evaluator.evaluate_retrieval(
            query=question,
            retrieved_documents=doc_contents
        )

        # 5. ì •ëŸ‰ì  ë©”íŠ¸ë¦­ ê³„ì‚° (embeddingsê°€ ìˆëŠ” ê²½ìš°)
        quantitative_metrics = {}
        try:
            # ì„ì‹œë¡œ í‰ê°€ìš© DataFrame ìƒì„±
            eval_df = pd.DataFrame([{
                'question': question,
                'answer': response,
                'ground_truth': ground_truth,
                'contexts': doc_contents
            }])

            # embeddings ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
            if hasattr(self.evaluator, 'embeddings') and self.evaluator.embeddings is not None:
                quantitative_metrics = self.evaluator.evaluate_response_all(eval_df)
            else:
                # embeddingsê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ë©”íŠ¸ë¦­ë§Œ ê³„ì‚°
                quantitative_metrics = {
                    'answer_similarity': self.evaluator.calculate_answer_similarity(response, ground_truth),
                    'faithfulness': self.evaluator.calculate_faithfulness(response, doc_contents)
                }
        except Exception as e:
            print(f"ì •ëŸ‰ì  ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            quantitative_metrics = {}

        # 6. ê²°ê³¼ ì¢…í•©
        evaluation_result = {
            'timestamp': datetime.now().isoformat(),
            'query_data': query_data,
            'response': response,
            'retrieved_docs_count': len(retrieved_docs),
            'response_evaluation': response_evaluation,
            'retrieval_evaluation': retrieval_evaluation,
            'quantitative_metrics': quantitative_metrics,
            'execution_time': None  # ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì¶”ê°€ ê°€ëŠ¥
        }

        return evaluation_result

    def run_comprehensive_evaluation(self, test_dataset: Optional[List[Dict]] = None) -> pd.DataFrame:
        """
        ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì¢…í•© í‰ê°€ ì‹¤í–‰

        Args:
            test_dataset: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (Noneì´ë©´ ê¸°ë³¸ ë°ì´í„°ì…‹ ì‚¬ìš©)

        Returns:
            í‰ê°€ ê²°ê³¼ DataFrame
        """
        if test_dataset is None:
            test_dataset = self.create_test_dataset()

        print(f"ğŸ“Š RAG ì‹œìŠ¤í…œ ì¢…í•© í‰ê°€ ì‹œì‘ (ì´ {len(test_dataset)}ê°œ ì¿¼ë¦¬)")
        print("=" * 60)

        for i, query_data in enumerate(tqdm(test_dataset, desc="í‰ê°€ ì§„í–‰ì¤‘")):
            print(f"\nğŸ” ì¿¼ë¦¬ {i+1}: {query_data['question'][:50]}...")

            try:
                result = self.evaluate_single_query(query_data)
                self.evaluation_results.append(result)

                # ê°„ë‹¨í•œ ê²°ê³¼ ì¶œë ¥
                if 'answer_similarity' in result['quantitative_metrics']:
                    similarity = result['quantitative_metrics']['answer_similarity']
                    print(f"   ğŸ“ˆ ë‹µë³€ ìœ ì‚¬ë„: {similarity:.3f}")

            except Exception as e:
                print(f"   âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
                continue

        print(f"\nâœ… í‰ê°€ ì™„ë£Œ! ì´ {len(self.evaluation_results)}ê°œ ê²°ê³¼")

        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        return self.create_results_dataframe()

    def create_results_dataframe(self) -> pd.DataFrame:
        """í‰ê°€ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        rows = []

        for result in self.evaluation_results:
            query_data = result['query_data']
            metrics = result['quantitative_metrics']

            row = {
                'question': query_data['question'],
                'query_type': query_data['query_type'],
                'difficulty': query_data['difficulty'],
                'response_length': len(result['response']),
                'retrieved_docs_count': result['retrieved_docs_count'],
                **metrics  # ì •ëŸ‰ì  ë©”íŠ¸ë¦­ ì¶”ê°€
            }
            rows.append(row)

        return pd.DataFrame(rows)

    def generate_evaluation_report(self, results_df: pd.DataFrame, save_path: str = None):
        """
        í‰ê°€ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± (ì‹œê°í™” í¬í•¨)

        Args:
            results_df: í‰ê°€ ê²°ê³¼ DataFrame
            save_path: ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ
        """
        print("\n" + "="*60)
        print("ğŸ“Š RAG ì‹œìŠ¤í…œ í‰ê°€ ë¦¬í¬íŠ¸")
        print("="*60)

        # 1. ì „ì²´ í†µê³„
        print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
        print(f"   â€¢ ì´ í‰ê°€ ì¿¼ë¦¬ ìˆ˜: {len(results_df)}")
        print(f"   â€¢ í‰ê·  ì‘ë‹µ ê¸¸ì´: {results_df['response_length'].mean():.1f} ë¬¸ì")
        print(f"   â€¢ í‰ê·  ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜: {results_df['retrieved_docs_count'].mean():.1f}")

        # 2. ì •ëŸ‰ì  ë©”íŠ¸ë¦­ í†µê³„
        numeric_columns = results_df.select_dtypes(include=[np.number]).columns
        metric_columns = [col for col in numeric_columns if col not in ['response_length', 'retrieved_docs_count']]

        if metric_columns:
            print(f"\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­ (í‰ê· ):")
            for metric in metric_columns:
                avg_score = results_df[metric].mean()
                print(f"   â€¢ {metric}: {avg_score:.3f}")

        # 3. ì¿¼ë¦¬ íƒ€ì…ë³„ ë¶„ì„
        if 'query_type' in results_df.columns:
            print(f"\nğŸ¯ ì¿¼ë¦¬ íƒ€ì…ë³„ ì„±ëŠ¥:")
            type_stats = results_df.groupby('query_type').agg({
                col: 'mean' for col in metric_columns
            }).round(3)
            print(type_stats.to_string())

        # 4. ë‚œì´ë„ë³„ ë¶„ì„
        if 'difficulty' in results_df.columns:
            print(f"\nâš¡ ë‚œì´ë„ë³„ ì„±ëŠ¥:")
            difficulty_stats = results_df.groupby('difficulty').agg({
                col: 'mean' for col in metric_columns
            }).round(3)
            print(difficulty_stats.to_string())

        # 5. ìƒì„¸ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“‹ ìƒì„¸ í‰ê°€ ê²°ê³¼:")
        for i, result in enumerate(self.evaluation_results[:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"\n--- ì¿¼ë¦¬ {i+1} ---")
            print(f"ì§ˆë¬¸: {result['query_data']['question']}")
            print(f"ì‘ë‹µ: {result['response'][:200]}...")
            print(f"ì‘ë‹µ í‰ê°€:\n{result['response_evaluation'][:300]}...")

        # 6. ì‹œê°í™” ìƒì„± ë° ì €ì¥
        saved_files = {}
        if self.enable_visualization and save_path:
            try:
                print(f"\nğŸ¨ ì‹œê°í™” ìƒì„± ì¤‘...")
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

                # í†µí•© ëŒ€ì‹œë³´ë“œ ì €ì¥
                dashboard_path = self.visualizer.save_unified_dashboard(
                    results_df,
                    detailed_results=self.evaluation_results,
                    save_dir="visualizations",
                    timestamp=timestamp
                )

                saved_files = {'unified_dashboard': dashboard_path}
                print(f"ğŸ“ˆ í†µí•© ëŒ€ì‹œë³´ë“œ ì €ì¥ ì™„ë£Œ: {dashboard_path}")

            except Exception as e:
                print(f"âŒ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return saved_files

    def show_interactive_visualizations(self, results_df: pd.DataFrame):
        """
        í†µí•© ëŒ€í™”í˜• ì‹œê°í™”ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ

        Args:
            results_df: í‰ê°€ ê²°ê³¼ DataFrame
        """
        if not self.enable_visualization:
            print("ì‹œê°í™”ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        print("\nğŸ¨ í†µí•© ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...")

        try:
            # í†µí•© HTML ëŒ€ì‹œë³´ë“œ ìƒì„±
            unified_html = self.visualizer.create_unified_dashboard(results_df, self.evaluation_results)

            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°
            import tempfile
            import webbrowser

            with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as f:
                f.write(unified_html)
                temp_path = f.name

            print(f"ğŸ“ˆ í†µí•© ëŒ€ì‹œë³´ë“œë¥¼ ë¸Œë¼ìš°ì €ì—ì„œ ì—¬ëŠ” ì¤‘: {temp_path}")
            webbrowser.open(f'file://{temp_path}')
            print("âœ… í†µí•© ëŒ€ì‹œë³´ë“œê°€ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ë ¸ìŠµë‹ˆë‹¤!")

        except Exception as e:
            print(f"âŒ ì‹œê°í™” í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {e}")

    def compare_configurations(self, configs: Dict[str, Dict]) -> pd.DataFrame:
        """
        ë‹¤ì–‘í•œ RAG ì„¤ì • ë¹„êµ í‰ê°€

        Args:
            configs: ì„¤ì • ì´ë¦„ê³¼ RAG ì‹œìŠ¤í…œ ì„¤ì • ë”•ì…”ë„ˆë¦¬

        Returns:
            ë¹„êµ ê²°ê³¼ DataFrame
        """
        print("\nğŸ”„ RAG ì„¤ì • ë¹„êµ í‰ê°€ ì‹œì‘")
        print("="*40)

        comparison_results = []
        test_dataset = self.create_test_dataset()

        for config_name, config_params in configs.items():
            print(f"\nğŸ“ {config_name} ì„¤ì • í‰ê°€ ì¤‘...")

            # ìƒˆë¡œìš´ RAG ì‹œìŠ¤í…œ ìƒì„±
            try:
                rag_system = RAGSystem(**config_params)
                evaluator = RAGEvaluationPipeline(rag_system, self.evaluator)

                # í‰ê°€ ì‹¤í–‰
                results_df = evaluator.run_comprehensive_evaluation(test_dataset)

                # ê²°ê³¼ ìš”ì•½
                numeric_columns = results_df.select_dtypes(include=[np.number]).columns
                metric_columns = [col for col in numeric_columns if col not in ['response_length', 'retrieved_docs_count']]

                config_result = {'configuration': config_name}
                for metric in metric_columns:
                    config_result[metric] = results_df[metric].mean()

                comparison_results.append(config_result)

            except Exception as e:
                print(f"âŒ {config_name} ì„¤ì • í‰ê°€ ì‹¤íŒ¨: {e}")
                continue

        comparison_df = pd.DataFrame(comparison_results)

        print("\nğŸ“Š ì„¤ì • ë¹„êµ ê²°ê³¼:")
        print(comparison_df.to_string(index=False))

        return comparison_df


def main():
    """RAG í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜ˆì‹œ"""

    # 1. RAG ì‹œìŠ¤í…œê³¼ í‰ê°€ì ì´ˆê¸°í™”
    print("ğŸš€ RAG í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")

    try:
        # RAG ì‹œìŠ¤í…œ ë¡œë“œ
        rag_system = RAGSystem()

        # LLM ì´ˆê¸°í™” (í‰ê°€ìš©)
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.1
        )

        # CustomEvaluator ì´ˆê¸°í™”
        evaluator = CustomEvaluatior(llm)

        # í‰ê°€ íŒŒì´í”„ë¼ì¸ ìƒì„±
        evaluation_pipeline = RAGEvaluationPipeline(rag_system, evaluator)

        # 2. ì¢…í•© í‰ê°€ ì‹¤í–‰
        print("\nğŸ“Š ì¢…í•© í‰ê°€ ì‹¤í–‰...")
        results_df = evaluation_pipeline.run_comprehensive_evaluation()

        # 3. í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = f"evaluation_results_{timestamp}"

        evaluation_pipeline.generate_evaluation_report(
            results_df,
            save_path=report_path
        )

        # 4. ì„¤ì • ë¹„êµ (ì„ íƒì‚¬í•­)
        print("\nğŸ”„ ë‹¤ì–‘í•œ ì„¤ì • ë¹„êµ...")
        configs = {
            "basic": {"use_reranking": False},
            "with_reranking": {"use_reranking": True},
            # "custom_weights": {"ensemble_weights": [0.8, 0.2]}  # í•„ìš”ì‹œ ì¶”ê°€
        }

        comparison_df = evaluation_pipeline.compare_configurations(configs)
        comparison_df.to_csv(f"rag_config_comparison_{timestamp}.csv", index=False)

        print(f"\nâœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼ë“¤ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()